{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 诗歌生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)  # 禁用 AutoGraph 的警告\n",
    "\n",
    "start_token = 'bos'\n",
    "end_token = 'eos'\n",
    "\n",
    "def process_dataset(fileName):\n",
    "    examples = []\n",
    "    with open(fileName, 'r') as fd: # 一行一首诗，以冒号分隔标题与内容\n",
    "        for line in fd: # 取每一行（对于每一首诗）\n",
    "            outs = line.strip().split(':') # 去除前后空格，将标题与内容分开\n",
    "            content = ''.join(outs[1:]) # 取出内容\n",
    "            ins = [start_token] + list(content) + [end_token]  # 内容加上开始结束标记\n",
    "            if len(ins) > 200: # 如果内容字数超过200则忽略\n",
    "                continue\n",
    "            examples.append(ins) # 否则加入样例列表\n",
    "            \n",
    "    counter = collections.Counter() # 用于对所有字符进行计数\n",
    "    for e in examples: # 对于处理过的每首诗的内容\n",
    "        for w in e: # 对于内容中的每个字符\n",
    "            counter[w]+=1 # 对饮计数+1\n",
    "    def negative_x(x):\n",
    "        return -x[1]\n",
    "    sorted_counter = sorted(counter.items(), key=negative_x)  # 字符按出现次数排序\n",
    "    # counter.items()返回一个列表，列表中的元素是字典键值对元组;\n",
    "    # sorted中key=lambda x: -x[1]表示按照第二个元素降序排列\n",
    "    words, _ = zip(*sorted_counter) \n",
    "    # 排序后的元组列表，首先\n",
    "    #  *为解包操作符，将列表中的元组解包:每个元组拆成一个key一个value；\n",
    "    #  zip()函数将多个可迭代对象压缩:所有元组的key压缩成一个元组，所有元组的value压缩成一个元组\n",
    "    # 只取了排序后的字符；出现次数用于排序后丢弃。\n",
    "    words = ('PAD', 'UNK') + words[:len(words)] # PAD是填充字符，UNK是未知字符\n",
    "    word2id = dict(zip(words, range(len(words)))) # 字符用在整个字符集中的索引代替\n",
    "    id2word = {word2id[k]:k for k in word2id} # 索引到字符的映射\n",
    "    \n",
    "    indexed_examples = [[word2id[w] for w in poem]\n",
    "                        for poem in examples] # 每首诗一个列表，原始内容的字符替换为索引\n",
    "    seqlen = [len(e) for e in indexed_examples] # 所有示例的诗的长度\n",
    "    \n",
    "    instances = list(zip(indexed_examples, seqlen)) # 内容索引和长度元组作为一个Instance\n",
    "    # list()将zip()返回的迭代器转换为列表\n",
    "    \n",
    "    return instances, word2id, id2word # 返回示例，字符到索引的映射，索引到字符的映射\n",
    "\n",
    "def poem_dataset():\n",
    "    # 诗数据（文字索引和长度），文字到索引映射、索引到文字映射\n",
    "    instances, word2id, id2word = process_dataset('./poems.txt')\n",
    "    # 生成器函数 输出类型 输出形状\n",
    "    # 输出形状（第一个张量的形状[None]，一维张量，长度可变；第二个[]，零维张量，标量）\n",
    "    def lst_instance():\n",
    "        return [ins for ins in instances]\n",
    "    ds = tf.data.Dataset.from_generator(lst_instance, \n",
    "                                            (tf.int64, tf.int64), \n",
    "                                            (tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.shuffle(buffer_size=10240) # 打乱数据集样本\n",
    "    ds = ds.padded_batch(100, padded_shapes=(tf.TensorShape([None]),tf.TensorShape([]))) # 数据集样本分批（数量100），每个批次样本进行填充使形状一致\n",
    "    def ds_map(x, seqlen):\n",
    "        return (x[:, :-1], x[:, 1:], seqlen-1)\n",
    "    ds = ds.map(ds_map) # 生成输入序列、输出序列，序列长度-1\n",
    "    return ds, word2id, id2word # 返回数据集（输入和目标序列），字符到索引的映射，索引到字符的映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNcell: (output:output1, next_state:h1) = call(input:x1, next_state:h0)\n",
    "- state_size\n",
    "- output_size\n",
    "dynamic_rnn: {h0, x1, x2, ……, xn} -> {h1, h2, ……, hn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型代码， 完成建模代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self, w2id):\n",
    "        # w2id：字符到索引的映射字典；\n",
    "        super(myRNNModel, self).__init__()\n",
    "        self.v_sz = len(w2id) # v_sz：字符集大小\n",
    "        self.embed_layer = tf.keras.layers.Embedding(self.v_sz, 64, \n",
    "                                                    batch_input_shape=[None, None])\n",
    "        # 嵌入层将输入的整数索引转换为固定大小的稠密向量 （64，嵌入向量的维度emb_sz）\n",
    "        # 输入的形状为[批次大小，序列长度]，None表示可变长度\n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(128) # 128:RNN单元的隐藏状态维度h_sz\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)# 使用RNN单元，返整个序列输出\n",
    "        self.dense = tf.keras.layers.Dense(self.v_sz) # 全连接层，输出维度为字符集大小\n",
    "        # 将RNN的输出映射到词汇表大小的logits\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inp_ids):\n",
    "        '''\n",
    "        此处完成建模过程，可以参考Learn2Carry\n",
    "        '''\n",
    "        # inp_ids: [b_sz, seq_len] 输入序列列\n",
    "        # inp_emb: [b_sz, seq_len, emb_sz] 嵌入输入序列\n",
    "        inp_emb = self.embed_layer(inp_ids)\n",
    "        # rnn_output: [b_sz, seq_len, h_sz] RNN层输出\n",
    "        rnn_output = self.rnn_layer(inp_emb) \n",
    "        # logits: [b_sz, seq_len, v_sz] 全连接输出层输出\n",
    "        logits = self.dense(rnn_output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def get_next_token(self, x, state):\n",
    "        '''\n",
    "        shape(x) = [b_sz,] \n",
    "        x: 输入\n",
    "        state: RNN的隐藏状态\n",
    "        '''\n",
    "        inp_emb = self.embed_layer(x) #嵌入输入：(b_sz,) -> (b_sz, emb_sz：64)\n",
    "        # 输入：嵌入输入+rnn当前状态；输出：当前时间步的隐藏状态+更新后的rnn\n",
    "        # state、h: shape(b_sz, h_sz：128)\n",
    "        h, state = self.rnncell.call(inp_emb, state) \n",
    "        logits = self.dense(h) # (b_sz, v_sz) keras的层输入维度会在第一次调用时自动推断\n",
    "        out = tf.argmax(logits, axis=-1) \n",
    "        return out, state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个计算sequence loss的辅助函数，只需了解用途。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkMask(input_tensor, maxLen):\n",
    "    shape_of_input = tf.shape(input_tensor) # 输入张量形状\n",
    "    shape_of_output = tf.concat(axis=0, values=[shape_of_input, [maxLen]]) \n",
    "\n",
    "    oneDtensor = tf.reshape(input_tensor, shape=(-1,))\n",
    "    flat_mask = tf.sequence_mask(oneDtensor, maxlen=maxLen)\n",
    "    return tf.reshape(flat_mask, shape_of_output)\n",
    "\n",
    "\n",
    "def reduce_avg(reduce_target, lengths, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reduce_target : shape(d_0, d_1,..,d_dim, .., d_k)\n",
    "        lengths : shape(d0, .., d_(dim-1))\n",
    "        dim : which dimension to average, should be a python number\n",
    "    在指定维度dim上进行加权平均\n",
    "    \"\"\"\n",
    "    # 检查输入向量\n",
    "    shape_of_lengths = lengths.get_shape()\n",
    "    shape_of_target = reduce_target.get_shape()\n",
    "    if len(shape_of_lengths) != dim:\n",
    "        raise ValueError(('Second input tensor should be rank %d, ' +\n",
    "                         'while it got rank %d') % (dim, len(shape_of_lengths)))\n",
    "    if len(shape_of_target) < dim+1 :\n",
    "        raise ValueError(('First input tensor should be at least rank %d, ' +\n",
    "                         'while it got rank %d') % (dim+1, len(shape_of_target)))\n",
    "\n",
    "    rank_diff = len(shape_of_target) - len(shape_of_lengths) - 1 # 计算秩差\n",
    "    # 生成掩码\n",
    "    mxlen = tf.shape(reduce_target)[dim] \n",
    "    mask = mkMask(lengths, mxlen) \n",
    "    if rank_diff!=0:\n",
    "        len_shape = tf.concat(axis=0, values=[tf.shape(lengths), [1]*rank_diff])\n",
    "        mask_shape = tf.concat(axis=0, values=[tf.shape(mask), [1]*rank_diff])\n",
    "    else:\n",
    "        len_shape = tf.shape(lengths)\n",
    "        mask_shape = tf.shape(mask)\n",
    "    lengths_reshape = tf.reshape(lengths, shape=len_shape)\n",
    "    mask = tf.reshape(mask, shape=mask_shape)\n",
    "    # 计算加权和\n",
    "    mask_target = reduce_target * tf.cast(mask, dtype=reduce_target.dtype)\n",
    "    red_sum = tf.reduce_sum(mask_target, axis=[dim], keepdims=False)\n",
    "    # 计算平均值\n",
    "    red_avg = red_sum / (tf.cast(lengths_reshape, dtype=tf.float32) + 1e-30)\n",
    "    return red_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义loss函数，定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels, seqlen):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    losses = reduce_avg(losses, seqlen, dim=1)\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def train_one_step(model, optimizer, x, y, seqlen):\n",
    "    # print(\"x shape:\", x.shape, \"x dtype:\", x.dtype)\n",
    "    '''\n",
    "    完成一步优化过程，可以参考之前做过的模型\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 前向传播\n",
    "        logits = model(x)\n",
    "        # 计算损失\n",
    "        loss = compute_loss(logits, y, seqlen)\n",
    "    # 计算损失对模型参数的梯度\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # 更新参数\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train(epoch, model, optimizer, ds):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step, (x, y, seqlen) in enumerate(ds):\n",
    "        loss = train_one_step(model, optimizer, x, y, seqlen)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print('epoch', epoch, ': loss', loss.numpy())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练优化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 8.820801\n",
      "epoch 1 : loss 6.610962\n",
      "epoch 2 : loss 6.23307\n",
      "epoch 3 : loss 5.90259\n",
      "epoch 4 : loss 5.6982703\n",
      "epoch 5 : loss 5.5709143\n",
      "epoch 6 : loss 5.4467945\n",
      "epoch 7 : loss 5.4175086\n",
      "epoch 8 : loss 5.3035693\n",
      "epoch 9 : loss 5.2295833\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(0.0005)\n",
    "train_ds, word2id, id2word = poem_dataset()\n",
    "model = myRNNModel(word2id)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train(epoch, model, optimizer, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三年不可知，此时无事事。eos生不可见，不见天中去。eos然不可忘，不见无人间。eos然不可见，不见不可知。eos子\n"
     ]
    }
   ],
   "source": [
    "def gen_sentence():\n",
    "    state = [tf.random.normal(shape=(1, 128), stddev=0.5), tf.random.normal(shape=(1, 128), stddev=0.5)]\n",
    "    cur_token = tf.constant([word2id['bos']], dtype=tf.int32)\n",
    "    collect = []\n",
    "    for _ in range(50):\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        collect.append(cur_token.numpy()[0])\n",
    "    return [id2word[t] for t in collect]\n",
    "print(''.join(gen_sentence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日日，一声何处不知。eos子不知何处事，不知何处是何人。eos道不知何处事，不知何处是何人。eos子不知何处事，不\n",
      "\n",
      "红叶滴红花满花。eos有一时无处处，一年何处不知人。eos来不得无人事，不得人间不可知。eos道不知何处事，不知何\n",
      "\n",
      "山边雨满江风。eos落花声落，风风落月深。eos来无处处，不见此中时。eos子无人事，何人不可知。eos来无处处，不见\n",
      "\n",
      "夜暮，一片月中春。eos色不知，此时何。eos子不知，此时何。eos子不知，不得之》）eos，一时不可知。eos中无处事，\n",
      "\n",
      "湖上，无人在何人。eos来不得无人事，不得人间不可知。eos道不知何处事，不知何处是何人。eos子不知何处事，不知\n",
      "\n",
      "海，今日无人不可知。eos道不知何处事，不知何处是何人。eos子不知何处事，不知何处是何人。eos子不知何处事，不\n",
      "\n",
      "月侵苔叶，风雨满花声。eos客无人事，何人不可知。eos来无处处，不见此中时。eos子无人事，何人不可知。eos来无处\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gen_sentence_for_begin_word(begin_word):\n",
    "    state = [tf.random.normal(shape=(1, 128), stddev=0.5), tf.random.normal(shape=(1, 128), stddev=0.5)]\n",
    "    cur_token = tf.constant([word2id[begin_word]], dtype=tf.int32)\n",
    "    collect = [cur_token.numpy()[0]]\n",
    "    for _ in range(50):\n",
    "        cur_token, state = model.get_next_token(cur_token, state)\n",
    "        collect.append(cur_token.numpy()[0])\n",
    "    return [id2word[t] for t in collect]\n",
    "print(''.join(gen_sentence_for_begin_word('日')) + '\\n')\n",
    "print(''.join(gen_sentence_for_begin_word('红'))+ '\\n')\n",
    "print(''.join(gen_sentence_for_begin_word('山'))+ '\\n')\n",
    "print(''.join(gen_sentence_for_begin_word('夜'))+ '\\n')\n",
    "print(''.join(gen_sentence_for_begin_word('湖'))+ '\\n')\n",
    "print(''.join(gen_sentence_for_begin_word('海'))+ '\\n')\n",
    "print(''.join(gen_sentence_for_begin_word('月'))+ '\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
